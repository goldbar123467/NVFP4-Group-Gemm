{
  "memories": [
    {
      "content": "NVFP4 Group GEMM Kernel Configuration (B200/Blackwell): The optimal configuration for NVFP4 block-scaled GEMM on NVIDIA B200 uses: mma_tiler_mnk=(128,128,256), mma_inst_shape_k=64, threads_per_cta=128, num_ab_stage=1 (NOT 3 - multi-stage was 30% SLOWER: 488µs vs 373µs), num_tmem_alloc_cols=512. Data types: ab_dtype=Float4E2M1FN, sf_dtype=Float8E4M3FN, c_dtype=Float16, sf_vec_size=16. The MmaMXF4NVF4Op from tcgen05 is used for Blackwell tensor cores.",
      "category": "pattern",
      "project": "nvfp4_group_gemm",
      "tags": ["cuda", "cutlass", "nvfp4", "gemm", "blackwell", "b200", "optimization"]
    },
    {
      "content": "NVFP4 Group GEMM Performance Benchmarks (v7-clean-20260124): Test results on B200 - (1) 8 groups, K=7168, M=64-248, N=4096: 433±0.9µs avg, 418µs best (target: 18.8µs); (2) 8 groups, K=2048, M=40-196, N=7168: 440±0.5µs avg, 431µs best (target: 10.7µs); (3) 2 groups, K=4096, M=192-320, N=3072: 191±0.5µs avg, 183µs best (target: 2.4µs); (4) 2 groups, K=1536, M=128-384, N=4096: 166±0.5µs avg, 158µs best (target: 1.5µs). Current implementation is ~20-100x slower than speed-of-light targets.",
      "category": "outcome",
      "project": "nvfp4_group_gemm",
      "tags": ["benchmark", "nvfp4", "gemm", "performance", "b200"]
    },
    {
      "content": "CuTe DSL Group GEMM Architecture Pattern: For variable-size group GEMM, use linearized block indexing via bidz dimension. Delinearize bidz to (group_idx, coord_x, coord_y) by iterating through cta_mn_list which contains (num_m_tiles, num_n_tiles) per group. Each CTA handles one tile, with total_num_clusters = sum of (ceil(M/128) * ceil(N/128)) across all groups. TMA descriptors are updated per-group using TensorMapManager with SMEM mode. Scale factors use tile_atom_to_shape_SF and blocked layout with atom_shape=((32,4),(16,4)), atom_stride=((16,4),(0,1)).",
      "category": "pattern",
      "project": "nvfp4_group_gemm",
      "tags": ["cute", "cutlass", "group-gemm", "tma", "architecture"]
    },
    {
      "content": "NVFP4 Scale Factor Memory Layout: For NVFP4 with FP8 scale factors, the blocked layout uses: atom_shape=((32,4),(sf_vec_size,4)) with atom_stride=((16,4),(0,1)). Scale factors are loaded via TMA with internal_type=Int16. S2T (shared-to-tensor memory) copy uses Cp4x32x128bOp. The tmem layout for scales is created via blockscaled_utils.make_tmem_layout_sfa/sfb. Scale factor pointers are stored in sfasfb_reordered_tensors which come pre-permuted from the input.",
      "category": "pattern",
      "project": "nvfp4_group_gemm",
      "tags": ["nvfp4", "scale-factors", "memory-layout", "tma", "cutlass"]
    },
    {
      "content": "NVFP4 Kernel Compilation Caching: For group GEMM with CuTe, cache compiled kernels by number of groups since problem_sizes is baked into compilation. Use cache_key=f\"{len(problem_sizes)}\" and store in _compiled_kernel_cache dict. The cute.compile() call takes problem_sizes as a compile-time constant List[Tuple[int,int,int,int]]. This allows reuse across invocations with same group count but different actual sizes (sizes are passed dynamically via tensor_of_problem_sizes).",
      "category": "pattern",
      "project": "nvfp4_group_gemm",
      "tags": ["cute", "compilation", "caching", "optimization"]
    },
    {
      "content": "NVFP4 Optimization Finding - Multi-Stage Pipelining Hurts Performance: Testing showed that increasing num_ab_stage from 1 to 3 made the kernel 30% SLOWER (488µs vs 373µs baseline). This is counterintuitive but likely due to: (1) increased shared memory pressure, (2) more complex pipeline synchronization, (3) kernel launch overhead dominates over memory latency hiding for these problem sizes. Keep num_ab_stage=1 for this workload.",
      "category": "outcome",
      "project": "nvfp4_group_gemm",
      "tags": ["optimization", "pipelining", "nvfp4", "performance", "counterintuitive"]
    }
  ],
  "import_instructions": "To import these memories into RAG Brain, run: curl -X POST http://localhost:8000/remember -H 'Content-Type: application/json' -d '{...}' for each memory, or write a script to iterate through this file."
}
